{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de1ed78",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "## Import necessary libraries\n",
    "In this code window, we initiate the data preparation process by importing essential libraries. The 'pandas' library is utilized for data manipulation, while 'numpy' provides support for numerical operations. Additionally, we import the 'calendar' module to work with date-related functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4abc337",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T12:24:55.026736700Z",
     "start_time": "2023-06-12T12:24:52.747363600Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce3848",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ac75f2c",
   "metadata": {},
   "source": [
    "## Load metadata for air quality locations\n",
    "Here, we retrieve metadata from a CSV file hosted online. This metadata contains information about various air quality monitoring locations. The data is fetched using the 'pd.read_csv' function from the 'pandas' library and stored in the 'meta_data' variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ddddc9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T12:28:08.614962600Z",
     "start_time": "2023-06-12T12:28:08.134245900Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_data = pd.read_csv(\"https://raw.githubusercontent.com/CopernicusAtmosphere/air-quality-covid19-response/master/CAMS_AQ_LOCATIONS_V1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bf5ade",
   "metadata": {},
   "source": [
    "## Reading and Concatenating Air Quality Data\n",
    "In this section, we read and compile air quality data spanning multiple years. The 'aq_data' dictionary is employed to store individual DataFrames for each year. Using a loop, data for each year is retrieved from online sources and appended to the dictionary. Finally, all the data is concatenated into a single DataFrame named 'aq_final'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa51d494",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-12T12:37:56.083536Z",
     "start_time": "2023-06-12T12:37:49.048968200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>basetime</th>\n",
       "      <th>city_id</th>\n",
       "      <th>NO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>PM10</th>\n",
       "      <th>PM2.5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>AQ001</td>\n",
       "      <td>25.28</td>\n",
       "      <td>30.06</td>\n",
       "      <td>41.76</td>\n",
       "      <td>19.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>AQ002</td>\n",
       "      <td>22.67</td>\n",
       "      <td>30.05</td>\n",
       "      <td>13.58</td>\n",
       "      <td>8.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>AQ003</td>\n",
       "      <td>7.80</td>\n",
       "      <td>63.02</td>\n",
       "      <td>7.39</td>\n",
       "      <td>4.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>AQ004</td>\n",
       "      <td>28.66</td>\n",
       "      <td>40.01</td>\n",
       "      <td>20.09</td>\n",
       "      <td>14.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>AQ005</td>\n",
       "      <td>14.80</td>\n",
       "      <td>35.78</td>\n",
       "      <td>50.96</td>\n",
       "      <td>28.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162695</th>\n",
       "      <td>2023-11-28</td>\n",
       "      <td>AQ046</td>\n",
       "      <td>6.42</td>\n",
       "      <td>82.89</td>\n",
       "      <td>25.46</td>\n",
       "      <td>7.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162696</th>\n",
       "      <td>2023-11-28</td>\n",
       "      <td>AQ047</td>\n",
       "      <td>20.64</td>\n",
       "      <td>23.63</td>\n",
       "      <td>14.94</td>\n",
       "      <td>12.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162697</th>\n",
       "      <td>2023-11-28</td>\n",
       "      <td>AQ048</td>\n",
       "      <td>16.39</td>\n",
       "      <td>35.26</td>\n",
       "      <td>15.87</td>\n",
       "      <td>11.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162698</th>\n",
       "      <td>2023-11-28</td>\n",
       "      <td>AQ049</td>\n",
       "      <td>26.41</td>\n",
       "      <td>16.89</td>\n",
       "      <td>41.56</td>\n",
       "      <td>35.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162699</th>\n",
       "      <td>2023-11-28</td>\n",
       "      <td>AQ050</td>\n",
       "      <td>21.75</td>\n",
       "      <td>23.93</td>\n",
       "      <td>23.55</td>\n",
       "      <td>24.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162700 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          basetime city_id    NO2     O3   PM10  PM2.5\n",
       "0       2015-01-01   AQ001  25.28  30.06  41.76  19.86\n",
       "1       2015-01-01   AQ002  22.67  30.05  13.58   8.95\n",
       "2       2015-01-01   AQ003   7.80  63.02   7.39   4.38\n",
       "3       2015-01-01   AQ004  28.66  40.01  20.09  14.16\n",
       "4       2015-01-01   AQ005  14.80  35.78  50.96  28.66\n",
       "...            ...     ...    ...    ...    ...    ...\n",
       "162695  2023-11-28   AQ046   6.42  82.89  25.46   7.78\n",
       "162696  2023-11-28   AQ047  20.64  23.63  14.94  12.28\n",
       "162697  2023-11-28   AQ048  16.39  35.26  15.87  11.63\n",
       "162698  2023-11-28   AQ049  26.41  16.89  41.56  35.22\n",
       "162699  2023-11-28   AQ050  21.75  23.93  23.55  24.06\n",
       "\n",
       "[162700 rows x 6 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = range(2015, 2024)\n",
    "\n",
    "# Create an empty dictionary to store aq_data\n",
    "aq_data = {}\n",
    "\n",
    "# Read aq_data for each year and store in aq_data dictionary\n",
    "for year in years:\n",
    "    url = f\"https://raw.githubusercontent.com/CopernicusAtmosphere/air-quality-covid19-response/master/cams_air_quality_analysis_{year}.csv\"\n",
    "    aq_data[str(year)] = pd.read_csv(url)\n",
    "\n",
    "# Concatenate all aq_data into a single DataFrame\n",
    "aq_final = pd.concat(aq_data.values(), ignore_index=True)\n",
    "\n",
    "aq_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb78c741",
   "metadata": {},
   "source": [
    "## Data Cleaning and Feature Engineering\n",
    "This code window focuses on cleaning and enhancing the air quality dataset Initial steps involve merging the data with the previously loaded metadata based on city IDs. Subsequently, rolling means for various time windows are computed to capture trends in NO2 concentrations. Monthly and yearly indices are then calculated to assess the relative changes in concentration over time. Finally, winners for both monthly and yearly challenges are determined based on the computed indices. The processed data is structured for further analysis and saved to a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c04da1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>year</th>\n",
       "      <th>Raw</th>\n",
       "      <th>365d moving average</th>\n",
       "      <th>Composite moving average</th>\n",
       "      <th>Monthly Index Challenge</th>\n",
       "      <th>Yearly Index Challenge</th>\n",
       "      <th>winner_month</th>\n",
       "      <th>winner_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>52.35</td>\n",
       "      <td>4.92</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>11.44</td>\n",
       "      <td>19.770822</td>\n",
       "      <td>20.001654</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>Sofia</td>\n",
       "      <td>Monaco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>52.35</td>\n",
       "      <td>4.92</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>13.17</td>\n",
       "      <td>19.757589</td>\n",
       "      <td>19.998284</td>\n",
       "      <td>99.983152</td>\n",
       "      <td>99.983152</td>\n",
       "      <td>Sofia</td>\n",
       "      <td>Monaco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>52.35</td>\n",
       "      <td>4.92</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>21.57</td>\n",
       "      <td>19.795589</td>\n",
       "      <td>20.004180</td>\n",
       "      <td>100.012631</td>\n",
       "      <td>100.012631</td>\n",
       "      <td>Sofia</td>\n",
       "      <td>Monaco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>52.35</td>\n",
       "      <td>4.92</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>21.74</td>\n",
       "      <td>19.806082</td>\n",
       "      <td>20.008093</td>\n",
       "      <td>100.032191</td>\n",
       "      <td>100.032191</td>\n",
       "      <td>Sofia</td>\n",
       "      <td>Monaco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>52.35</td>\n",
       "      <td>4.92</td>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>11.72</td>\n",
       "      <td>19.774274</td>\n",
       "      <td>19.990328</td>\n",
       "      <td>99.943377</td>\n",
       "      <td>99.943377</td>\n",
       "      <td>Sofia</td>\n",
       "      <td>Monaco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162695</th>\n",
       "      <td>Zagreb</td>\n",
       "      <td>45.80</td>\n",
       "      <td>16.00</td>\n",
       "      <td>2023-11-24</td>\n",
       "      <td>18.54</td>\n",
       "      <td>10.426329</td>\n",
       "      <td>11.656554</td>\n",
       "      <td>99.273091</td>\n",
       "      <td>92.580266</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162696</th>\n",
       "      <td>Zagreb</td>\n",
       "      <td>45.80</td>\n",
       "      <td>16.00</td>\n",
       "      <td>2023-11-25</td>\n",
       "      <td>10.12</td>\n",
       "      <td>10.387507</td>\n",
       "      <td>11.634742</td>\n",
       "      <td>99.087330</td>\n",
       "      <td>92.407029</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162697</th>\n",
       "      <td>Zagreb</td>\n",
       "      <td>45.80</td>\n",
       "      <td>16.00</td>\n",
       "      <td>2023-11-26</td>\n",
       "      <td>14.32</td>\n",
       "      <td>10.368986</td>\n",
       "      <td>11.625292</td>\n",
       "      <td>99.006843</td>\n",
       "      <td>92.331969</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162698</th>\n",
       "      <td>Zagreb</td>\n",
       "      <td>45.80</td>\n",
       "      <td>16.00</td>\n",
       "      <td>2023-11-27</td>\n",
       "      <td>16.40</td>\n",
       "      <td>10.369479</td>\n",
       "      <td>11.623249</td>\n",
       "      <td>98.989449</td>\n",
       "      <td>92.315747</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162699</th>\n",
       "      <td>Zagreb</td>\n",
       "      <td>45.80</td>\n",
       "      <td>16.00</td>\n",
       "      <td>2023-11-28</td>\n",
       "      <td>21.75</td>\n",
       "      <td>10.389288</td>\n",
       "      <td>11.631247</td>\n",
       "      <td>99.057565</td>\n",
       "      <td>92.379271</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>89650 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             city    lat    lng       year    Raw  365d moving average  \\\n",
       "1461    Amsterdam  52.35   4.92 2019-01-01  11.44            19.770822   \n",
       "1462    Amsterdam  52.35   4.92 2019-01-02  13.17            19.757589   \n",
       "1463    Amsterdam  52.35   4.92 2019-01-03  21.57            19.795589   \n",
       "1464    Amsterdam  52.35   4.92 2019-01-04  21.74            19.806082   \n",
       "1465    Amsterdam  52.35   4.92 2019-01-05  11.72            19.774274   \n",
       "...           ...    ...    ...        ...    ...                  ...   \n",
       "162695     Zagreb  45.80  16.00 2023-11-24  18.54            10.426329   \n",
       "162696     Zagreb  45.80  16.00 2023-11-25  10.12            10.387507   \n",
       "162697     Zagreb  45.80  16.00 2023-11-26  14.32            10.368986   \n",
       "162698     Zagreb  45.80  16.00 2023-11-27  16.40            10.369479   \n",
       "162699     Zagreb  45.80  16.00 2023-11-28  21.75            10.389288   \n",
       "\n",
       "        Composite moving average  Monthly Index Challenge  \\\n",
       "1461                   20.001654               100.000000   \n",
       "1462                   19.998284                99.983152   \n",
       "1463                   20.004180               100.012631   \n",
       "1464                   20.008093               100.032191   \n",
       "1465                   19.990328                99.943377   \n",
       "...                          ...                      ...   \n",
       "162695                 11.656554                99.273091   \n",
       "162696                 11.634742                99.087330   \n",
       "162697                 11.625292                99.006843   \n",
       "162698                 11.623249                98.989449   \n",
       "162699                 11.631247                99.057565   \n",
       "\n",
       "        Yearly Index Challenge winner_month winner_year  \n",
       "1461                100.000000        Sofia      Monaco  \n",
       "1462                 99.983152        Sofia      Monaco  \n",
       "1463                100.012631        Sofia      Monaco  \n",
       "1464                100.032191        Sofia      Monaco  \n",
       "1465                 99.943377        Sofia      Monaco  \n",
       "...                        ...          ...         ...  \n",
       "162695               92.580266                           \n",
       "162696               92.407029                           \n",
       "162697               92.331969                           \n",
       "162698               92.315747                           \n",
       "162699               92.379271                           \n",
       "\n",
       "[89650 rows x 11 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all aq_data into a single DataFrame\n",
    "aq_final = pd.concat(aq_data.values(), ignore_index=True)\n",
    "\n",
    "# Merge aq_final with meta_data on 'city_id' and 'id'\n",
    "aq_final = pd.merge(aq_final, meta_data[['id', 'name', 'latitude', 'longitude']], left_on='city_id', right_on='id')\n",
    "\n",
    "# Compute 'no2_rmean7' using rolling mean for each 'name'\n",
    "aq_final['no2_rmean7'] = aq_final.groupby('name')['NO2'].rolling(window=7, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "# Compute 'no2_rmean35' using rolling mean for each 'name'\n",
    "aq_final['no2_rmean35'] = aq_final.groupby('name')['NO2'].rolling(window=35, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "# Compute 'no2_rmean1j' using rolling mean for each 'name'\n",
    "aq_final['no2_rmean1j'] = aq_final.groupby('name')['NO2'].rolling(window=365, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "# Compute 'no2_rmean2j' using rolling mean for each 'name'\n",
    "aq_final['no2_rmean2j'] = aq_final.groupby('name')['NO2'].rolling(window=365*2, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "# Compute 'no2_rmean3j' using rolling mean for each 'name'\n",
    "aq_final['no2_rmean3j'] = aq_final.groupby('name')['NO2'].rolling(window=365*3, min_periods=1).mean().reset_index(0, drop=True)\n",
    "\n",
    "# Compute 'no2_rmean_c' as the weighted average of 'no2_rmean1j', 'no2_rmean2j', and 'no2_rmean3j' for each 'name'\n",
    "weights = np.array([0.2, 0.3, 0.5])\n",
    "\n",
    "aq_final['no2_rmean_c'] = (aq_final['no2_rmean1j'] * weights[0] +\n",
    "                           aq_final['no2_rmean2j'] * weights[1] +\n",
    "                           aq_final['no2_rmean3j'] * weights[2])\n",
    "\n",
    "# Convert 'basetime' column to datetime type\n",
    "aq_final['basetime'] = pd.to_datetime(aq_final['basetime'])\n",
    "aq_final = aq_final[aq_final['basetime'].dt.year >= 2019]\n",
    "\n",
    "# Group the data by 'name' and the year-month of 'basetime'\n",
    "grouped_m = aq_final.groupby(['name', aq_final['basetime'].dt.to_period('M')])\n",
    "\n",
    "# Calculate the index as the value of each day divided by the value of the first day of the month, multiplied by 100\n",
    "aq_final['monthly_index'] = grouped_m['no2_rmean_c'].transform(lambda x: x / x.iloc[0] * 100)\n",
    "\n",
    "# Group the data by 'name' and the year of 'basetime'\n",
    "grouped_y = aq_final.groupby(['name', aq_final['basetime'].dt.year])\n",
    "\n",
    "# Calculate the index as the value of each day divided by the value of the first day of the year, multiplied by 100\n",
    "aq_final['yearly_index'] = grouped_y['no2_rmean_c'].transform(lambda x: x / x.iloc[0] * 100)\n",
    "\n",
    "# Rename columns\n",
    "# aq_final = aq_final.rename(columns={'no2_rmean1j': '365d moving average', 'no2_rmean_c': 'Composite moving average'})\n",
    "aq_final = aq_final.drop(['no2_rmean7', 'no2_rmean35', 'no2_rmean2j', 'no2_rmean3j'], axis=1)\n",
    "\n",
    "aq_final['days_in_month'] = aq_final['basetime'].dt.month.apply(lambda x: calendar.monthrange(pd.to_datetime('today').year, x)[1])\n",
    "\n",
    "# Determine the winner for each month\n",
    "aq_final['winner_month'] = \"\"\n",
    "for year in aq_final['basetime'].dt.year.unique():\n",
    "    for month in aq_final['basetime'].dt.month.unique():\n",
    "        month_data = aq_final[(aq_final['basetime'].dt.year == year) & (aq_final['basetime'].dt.month == month)]\n",
    "        last_day_of_month = month_data['basetime'].dt.day.max()\n",
    "        #last_day_of_month = calendar.monthrange(year, month)[1]\n",
    "        days_in_month = month_data['days_in_month'].max()\n",
    "        if last_day_of_month == days_in_month:\n",
    "            last_day_data = month_data[month_data['basetime'] == month_data['basetime'].max()]\n",
    "            if len(last_day_data) > 0:\n",
    "                min_monthly_index = last_day_data['monthly_index'].min()\n",
    "                winner_city_month = last_day_data[last_day_data['monthly_index'] == min_monthly_index]['name'].values[0]\n",
    "                aq_final.loc[(aq_final['basetime'].dt.year == year) & (aq_final['basetime'].dt.month == month), 'winner_month'] = winner_city_month\n",
    "\n",
    "# Determine the winner for each year\n",
    "aq_final['winner_year'] = \"\"\n",
    "for year in aq_final['basetime'].dt.year.unique():\n",
    "    year_data = aq_final[aq_final['basetime'].dt.year == year]\n",
    "    max_date = year_data['basetime'].max()  \n",
    "    if max_date.month == 12 and max_date.day == 31:\n",
    "        last_day_data = year_data[year_data['basetime'] == year_data['basetime'].max()]\n",
    "        if len(last_day_data) > 0:\n",
    "            min_yearly_index = last_day_data['yearly_index'].min()\n",
    "            winner_city_year = last_day_data[last_day_data['yearly_index'] == min_yearly_index]['name'].values[0]\n",
    "            aq_final.loc[aq_final['basetime'].dt.year == year, 'winner_year'] = winner_city_year\n",
    "\n",
    "aq_final_csv = aq_final[['name', 'latitude', 'longitude', 'basetime', 'NO2', 'no2_rmean1j', 'no2_rmean_c', 'monthly_index', 'yearly_index', 'winner_month', 'winner_year']].copy()\n",
    "aq_final_csv.columns = ['city', 'lat', 'lng', 'year', 'Raw', '365d moving average', 'Composite moving average', \"Monthly Index Challenge\", \"Yearly Index Challenge\", 'winner_month', 'winner_year']\n",
    "\n",
    "aq_final_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b7b52d",
   "metadata": {},
   "source": [
    "## Saving Processed Data to CSV\n",
    "writing the resulting DataFrame to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c86aacb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aq_final_csv.to_csv('../static/data/data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
